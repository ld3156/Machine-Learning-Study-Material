{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eebeea33-ac18-4173-ae78-8e8cda2a13a5",
   "metadata": {},
   "source": [
    "# IBM Machine Learning Online Course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e82e3-98c3-42f9-90c5-aa985104d891",
   "metadata": {},
   "source": [
    "**Course Outline:**\n",
    "\n",
    "Module 1: Introduction to Machine Learning\n",
    "\n",
    "Knowledge of foundational machine learning concepts; you will be introduced to various open-source tools for machine learning, including the popular Python package scikit-learn.\n",
    "\n",
    "Module 2: Linear and Logistic Regression\n",
    "\n",
    "two classical statistical methods foundational to machine learning: linear and logistic regression.\n",
    "\n",
    "Module 3: Building Supervised Learning Models\n",
    "\n",
    "You will start by understanding how binary classification works and discover how to construct a multiclass classifier from binary classification components. You’ll learn what decision trees are, how they learn, and how to build them. Decision trees, which are used to solve classification problems, have a natural extension called regression trees, which can handle regression problems. You’ll learn about other supervised learning models, like KNN and SVM. You’ll learn what bias and variance are in model fitting and the tradeoff between bias and variance that is inherent to all learning models in various degrees. You’ll learn strategies for mitigating this tradeoff and work with models that do a very good job accomplishing that goal.\n",
    "\n",
    "Module 4: Building Unsupervised Learning Models\n",
    "\n",
    "In this module, you’ll dive into unsupervised learning, where algorithms uncover patterns in data without labeled examples. You’ll explore clustering strategies and real-world applications, focusing on techniques like hierarchical clustering, k-means, and advanced methods such as DBSCAN and HDBSCAN. Through practical labs, you’ll gain a deeper understanding of how to compare and implement these algorithms effectively. Additionally, you’ll delve into dimension reduction algorithms like PCA (Principal Component Analysis), t-SNE, and UMAP to reduce dataset features and simplify other modeling tasks. Using Python, you’ll implement these clustering and dimensionality reduction techniques, learning how to integrate them with feature engineering to prepare data for machine learning models.\n",
    "\n",
    "Module 5: Evaluating and Validating Machine Learning Models\n",
    "\n",
    "This module covers how to assess model performance on unseen data, starting with key evaluation metrics for classification and regression. You’ll also explore hyperparameter tuning to optimize models while avoiding overfitting using cross-validation. Special techniques, such as regularization in linear regression, will be introduced to handle overfitting due to outliers. Hands-on exercises in Python will guide you through model fine-tuning and cross-validation for reliable model evaluation.\n",
    "\n",
    "Module 6: Final Exam and Project\n",
    "\n",
    "In this concluding module, you’ll review the course content, complete a final exam, and work on a hands-on project. You’ll receive a course summary cheat sheet, apply your skills in a project on Rain Prediction in Australia, and participate in peer reviews to share feedback. The module wraps up with guidance on next steps in your learning journey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d3e89c-64c2-456f-a9c4-07772ca4a9ba",
   "metadata": {},
   "source": [
    "Data processing and analytics: PostgreSQL, Hadoop, Spark, Apache Kafka, pandas, and NumPy\n",
    "\n",
    "• Data visualization: Matplotlib, Seaborn, ggplot2, and Tableau\n",
    "\n",
    "• Machine learning: NumPy, Pandas, SciPy, and scikit-learn\n",
    "\n",
    "• Deep learning: TensorFlow, Keras, Theano, and PyTorch\n",
    "\n",
    "• Computer vision: OpenCV, scikit-image, and TorchVision\n",
    "\n",
    "• NLP: NLTK, TextBlob, and Stanza\n",
    "\n",
    "• Generative AI: Hugging face transformers, ChatGPT, DALL-E, and PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758ea4fd-b994-4450-935c-c692b15060e7",
   "metadata": {},
   "source": [
    "### Ch1.Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f8d67-943f-4860-ba70-40669ea57a37",
   "metadata": {},
   "source": [
    "**machine learning solve questions using data**\n",
    "Classification, Regression, Clustering, Association, Anomaly detection, Sequence mining, Dimension reduction, Recommendation systems\n",
    "\n",
    "Problem Definition, Data Collection, Data Preparation, Model Development, Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61f23998-74ce-46c0-9f35-4f6ab1a48d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b5be42-4fe4-47cb-811b-66945b53e98d",
   "metadata": {},
   "source": [
    "Example code:\n",
    "\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33)\n",
    "\n",
    "clf = svm.SVC(gamma = 0.001, C = 100.)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b03ee09-bf6b-49d4-a408-cc77277a978c",
   "metadata": {},
   "source": [
    "Artificial intelligence (AI) simulates human cognition, while **machine learning (ML) uses algorithms and requires feature engineering to learn from data**.\n",
    "\n",
    "Machine learning includes different types of models: **supervised learning**, which uses labeled data to make predictions; **unsupervised learning**, which finds patterns in unlabeled data; and **semi-supervised learning**, which trains on a small subset of labeled data.\n",
    "\n",
    "Key factors for choosing a machine learning technique include the type of problem to be solved, the available data, available resources, and the desired outcome.\n",
    "\n",
    "Machine learning techniques include **anomaly detection** for identifying unusual cases like fraud, **classification** for categorizing new data, **regression** for **predicting** continuous values, and **clustering** for grouping similar data points without labels.\n",
    "\n",
    "Machine learning tools support pipelines with modules for data preprocessing, model building, evaluation, optimization, and deployment.\n",
    "\n",
    "R is commonly used in machine learning for statistical analysis and data exploration, while Python offers a vast array of libraries for different machine learning tasks. Other programming languages used in ML include Julia, Scala, Java, and JavaScript, each suited to specific applications like high-performance computing and web-based ML models.\n",
    "\n",
    "Data visualization tools such as **Matplotlib** and **Seaborn** create customizable plots, **ggplot2** enables building graphics in layers, and Tableau provides interactive data dashboards.\n",
    "\n",
    "Python libraries commonly used in machine learning include NumPy for numerical computations, Pandas for data analysis and preparation, SciPy for scientific computing, and Scikit-learn for building traditional machine learning models.\n",
    "\n",
    "Deep learning frameworks such as TensorFlow, Keras, Theano, and PyTorch support the design, training, and testing of neural networks used in areas like computer vision and natural language processing.\n",
    "\n",
    "Computer vision tools enable applications like object detection, image classification, and facial recognition, while natural language processing (NLP) tools like **NLTK, TextBlob, and Stanza** facilitate text processing, sentiment analysis, and language parsing.\n",
    "\n",
    "Generative AI tools use artificial intelligence to create new content, including text, images, music, and other media, based on input data or prompts.\n",
    "\n",
    "Scikit-learn provides a range of functions, including classification, regression, clustering, data preprocessing, model evaluation, and exporting models for production use.\n",
    "\n",
    "The machine learning ecosystem includes a network of tools, frameworks, libraries, platforms, and processes that collectively support the development and management of machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb1da25-d577-47a2-afd1-3400d1751fe3",
   "metadata": {},
   "source": [
    "### Ch2.Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5cd871-cfb6-48c1-aec2-cc5a00738d9c",
   "metadata": {},
   "source": [
    "[SimpleLinearRegression](Simple-Linear-Regression.ipynb)\n",
    "\n",
    "[MultipleLinearRegression](Mulitple-Linear-Regression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0726975-a887-4c3d-a172-559e29cadbfe",
   "metadata": {},
   "source": [
    "[LogisticRegression](Logistic_Regression.ipynb)\n",
    "Logistic regression predict the probability whether something will happen.\n",
    "Using Sigmoid function. **Probaility predictor and a binary classifier.**\n",
    "\n",
    "To train a logistic regression. We first choose a set of parameters, then we predict the probability that class = 1. Then we calculate prediction error (cost function), finally we update the parameters to reduce prediction error. **Log-loss function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fa423b-a21e-4dc1-aa6c-f781b38adca6",
   "metadata": {},
   "source": [
    "Simple regression uses a single independent variable to estimate a dependent variable, while multiple regression involves more than one independent variable.\n",
    "\n",
    "**In simple linear regression, a best-fit line minimizes errors, measured by Mean Squared Error (MSE); this approach is known as Ordinary Least Squares (OLS).**\n",
    "\n",
    "OLS regression is easy to interpret but **sensitive to outliers**, which can impact accuracy.\n",
    "\n",
    "Multiple linear regression extends simple linear regression by using multiple variables to predict outcomes and analyze variable relationships.\n",
    "\n",
    "Adding too many variables can lead to **overfitting**, so careful variable selection is necessary to build a balanced model.\n",
    "\n",
    "**Nonlinear regressio** models complex relationships using **polynomial, exponential, or logarithmic** functions when data does not fit a straight line.\n",
    "\n",
    "Polynomial regression can fit data but mayoverfit by capturing random noise rather than underlying patterns.\n",
    "\n",
    "Logistic regression is a probability predictor and binary classifier, suitable for **binary targets and assessing feature impact**.\n",
    "\n",
    "Logistic regression **minimizes errors using log-loss** and optimizes with gradient descent or stochastic gradient descent for efficiency.\n",
    "\n",
    "**Gradient descent** is an iterative process to minimize the cost function, which is crucial for training logistic regression models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1bf267-6ffb-400b-85a5-5caf5860328d",
   "metadata": {},
   "source": [
    "**Reference (Linear and Logistic Regression Cheatsheet.pdf)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56834eaa-0d62-4856-a666-9285683e65f4",
   "metadata": {},
   "source": [
    "### Ch3. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f2ed64-2c54-40cd-8bb0-9dddae12fe4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
