{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eebeea33-ac18-4173-ae78-8e8cda2a13a5",
   "metadata": {},
   "source": [
    "# IBM Machine Learning Online Course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283c6447-49e4-467a-9099-8cce2aea6e14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Course Outline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e82e3-98c3-42f9-90c5-aa985104d891",
   "metadata": {},
   "source": [
    "Module 1: Introduction to Machine Learning\n",
    "\n",
    "Knowledge of foundational machine learning concepts; you will be introduced to various open-source tools for machine learning, including the popular Python package scikit-learn.\n",
    "\n",
    "Module 2: Linear and Logistic Regression\n",
    "\n",
    "two classical statistical methods foundational to machine learning: linear and logistic regression.\n",
    "\n",
    "Module 3: Building Supervised Learning Models\n",
    "\n",
    "You will start by understanding how binary classification works and discover how to construct a multiclass classifier from binary classification components. You’ll learn what decision trees are, how they learn, and how to build them. Decision trees, which are used to solve classification problems, have a natural extension called regression trees, which can handle regression problems. You’ll learn about other supervised learning models, like KNN and SVM. You’ll learn what bias and variance are in model fitting and the tradeoff between bias and variance that is inherent to all learning models in various degrees. You’ll learn strategies for mitigating this tradeoff and work with models that do a very good job accomplishing that goal.\n",
    "\n",
    "Module 4: Building Unsupervised Learning Models\n",
    "\n",
    "In this module, you’ll dive into unsupervised learning, where algorithms uncover patterns in data without labeled examples. You’ll explore clustering strategies and real-world applications, focusing on techniques like hierarchical clustering, k-means, and advanced methods such as DBSCAN and HDBSCAN. Through practical labs, you’ll gain a deeper understanding of how to compare and implement these algorithms effectively. Additionally, you’ll delve into dimension reduction algorithms like PCA (Principal Component Analysis), t-SNE, and UMAP to reduce dataset features and simplify other modeling tasks. Using Python, you’ll implement these clustering and dimensionality reduction techniques, learning how to integrate them with feature engineering to prepare data for machine learning models.\n",
    "\n",
    "Module 5: Evaluating and Validating Machine Learning Models\n",
    "\n",
    "This module covers how to assess model performance on unseen data, starting with key evaluation metrics for classification and regression. You’ll also explore hyperparameter tuning to optimize models while avoiding overfitting using cross-validation. Special techniques, such as regularization in linear regression, will be introduced to handle overfitting due to outliers. Hands-on exercises in Python will guide you through model fine-tuning and cross-validation for reliable model evaluation.\n",
    "\n",
    "Module 6: Final Exam and Project\n",
    "\n",
    "In this concluding module, you’ll review the course content, complete a final exam, and work on a hands-on project. You’ll receive a course summary cheat sheet, apply your skills in a project on Rain Prediction in Australia, and participate in peer reviews to share feedback. The module wraps up with guidance on next steps in your learning journey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d3e89c-64c2-456f-a9c4-07772ca4a9ba",
   "metadata": {},
   "source": [
    "Data processing and analytics: PostgreSQL, Hadoop, Spark, Apache Kafka, pandas, and NumPy\n",
    "\n",
    "• Data visualization: Matplotlib, Seaborn, ggplot2, and Tableau\n",
    "\n",
    "• Machine learning: NumPy, Pandas, SciPy, and scikit-learn\n",
    "\n",
    "• Deep learning: TensorFlow, Keras, Theano, and PyTorch\n",
    "\n",
    "• Computer vision: OpenCV, scikit-image, and TorchVision\n",
    "\n",
    "• NLP: NLTK, TextBlob, and Stanza\n",
    "\n",
    "• Generative AI: Hugging face transformers, ChatGPT, DALL-E, and PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758ea4fd-b994-4450-935c-c692b15060e7",
   "metadata": {},
   "source": [
    "### Ch1.Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15f8d67-943f-4860-ba70-40669ea57a37",
   "metadata": {},
   "source": [
    "**machine learning solve questions using data**\n",
    "Classification, Regression, Clustering, Association, Anomaly detection, Sequence mining, Dimension reduction, Recommendation systems\n",
    "\n",
    "Problem Definition, Data Collection, Data Preparation, Model Development, Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61f23998-74ce-46c0-9f35-4f6ab1a48d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b5be42-4fe4-47cb-811b-66945b53e98d",
   "metadata": {},
   "source": [
    "Example code:\n",
    "\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33)\n",
    "\n",
    "clf = svm.SVC(gamma = 0.001, C = 100.)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b03ee09-bf6b-49d4-a408-cc77277a978c",
   "metadata": {},
   "source": [
    "Artificial intelligence (AI) simulates human cognition, while **machine learning (ML) uses algorithms and requires feature engineering to learn from data**.\n",
    "\n",
    "Machine learning includes different types of models: **supervised learning**, which uses labeled data to make predictions; **unsupervised learning**, which finds patterns in unlabeled data; and **semi-supervised learning**, which trains on a small subset of labeled data.\n",
    "\n",
    "Key factors for choosing a machine learning technique include the type of problem to be solved, the available data, available resources, and the desired outcome.\n",
    "\n",
    "Machine learning techniques include **anomaly detection** for identifying unusual cases like fraud, **classification** for categorizing new data, **regression** for **predicting** continuous values, and **clustering** for grouping similar data points without labels.\n",
    "\n",
    "Machine learning tools support pipelines with modules for data preprocessing, model building, evaluation, optimization, and deployment.\n",
    "\n",
    "R is commonly used in machine learning for statistical analysis and data exploration, while Python offers a vast array of libraries for different machine learning tasks. Other programming languages used in ML include Julia, Scala, Java, and JavaScript, each suited to specific applications like high-performance computing and web-based ML models.\n",
    "\n",
    "Data visualization tools such as **Matplotlib** and **Seaborn** create customizable plots, **ggplot2** enables building graphics in layers, and Tableau provides interactive data dashboards.\n",
    "\n",
    "Python libraries commonly used in machine learning include NumPy for numerical computations, Pandas for data analysis and preparation, SciPy for scientific computing, and Scikit-learn for building traditional machine learning models.\n",
    "\n",
    "Deep learning frameworks such as TensorFlow, Keras, Theano, and PyTorch support the design, training, and testing of neural networks used in areas like computer vision and natural language processing.\n",
    "\n",
    "Computer vision tools enable applications like object detection, image classification, and facial recognition, while natural language processing (NLP) tools like **NLTK, TextBlob, and Stanza** facilitate text processing, sentiment analysis, and language parsing.\n",
    "\n",
    "Generative AI tools use artificial intelligence to create new content, including text, images, music, and other media, based on input data or prompts.\n",
    "\n",
    "Scikit-learn provides a range of functions, including classification, regression, clustering, data preprocessing, model evaluation, and exporting models for production use.\n",
    "\n",
    "The machine learning ecosystem includes a network of tools, frameworks, libraries, platforms, and processes that collectively support the development and management of machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb1da25-d577-47a2-afd1-3400d1751fe3",
   "metadata": {},
   "source": [
    "### Ch2.Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5cd871-cfb6-48c1-aec2-cc5a00738d9c",
   "metadata": {},
   "source": [
    "[SimpleLinearRegression](Simple-Linear-Regression.ipynb)\n",
    "\n",
    "[MultipleLinearRegression](Mulitple-Linear-Regression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0726975-a887-4c3d-a172-559e29cadbfe",
   "metadata": {},
   "source": [
    "[LogisticRegression](Logistic_Regression.ipynb)\n",
    "Logistic regression predict the probability whether something will happen.\n",
    "Using Sigmoid function. **Probaility predictor and a binary classifier.**\n",
    "\n",
    "To train a logistic regression. We first choose a set of parameters, then we predict the probability that class = 1. Then we calculate prediction error (cost function), finally we update the parameters to reduce prediction error. **Log-loss function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fa423b-a21e-4dc1-aa6c-f781b38adca6",
   "metadata": {},
   "source": [
    "Simple regression uses a single independent variable to estimate a dependent variable, while multiple regression involves more than one independent variable.\n",
    "\n",
    "**In simple linear regression, a best-fit line minimizes errors, measured by Mean Squared Error (MSE); this approach is known as Ordinary Least Squares (OLS).**\n",
    "\n",
    "OLS regression is easy to interpret but **sensitive to outliers**, which can impact accuracy.\n",
    "\n",
    "Multiple linear regression extends simple linear regression by using multiple variables to predict outcomes and analyze variable relationships.\n",
    "\n",
    "Adding too many variables can lead to **overfitting**, so careful variable selection is necessary to build a balanced model.\n",
    "\n",
    "**Nonlinear regressio** models complex relationships using **polynomial, exponential, or logarithmic** functions when data does not fit a straight line.\n",
    "\n",
    "Polynomial regression can fit data but mayoverfit by capturing random noise rather than underlying patterns.\n",
    "\n",
    "Logistic regression is a probability predictor and binary classifier, suitable for **binary targets and assessing feature impact**.\n",
    "\n",
    "Logistic regression **minimizes errors using log-loss** and optimizes with gradient descent or stochastic gradient descent for efficiency.\n",
    "\n",
    "**Gradient descent** is an iterative process to minimize the cost function, which is crucial for training logistic regression models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1bf267-6ffb-400b-85a5-5caf5860328d",
   "metadata": {},
   "source": [
    "**Reference (Linear and Logistic Regression Cheatsheet.pdf)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56834eaa-0d62-4856-a666-9285683e65f4",
   "metadata": {},
   "source": [
    "### Ch3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdc4e4e-51f9-49e8-8093-b4c0b9001824",
   "metadata": {},
   "source": [
    "**One-vs-one**: Is it this or it is that? **One-vs-all**: Independent binary classifiers for each category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7916c6a-1d3c-4cc7-886b-cde0c2c649c1",
   "metadata": {},
   "source": [
    "**Decision Tree** [Decision Tree](Decision-tree-classifier-drug-pred-v1.ipynb)  considering the features one by one\n",
    "\n",
    "In a decision tree, **each internal node corresponds to a test, each branch corresponds to the result of the test, each terminal, or leaf node assigns its data to a class**. For example, you have several features, you start with a seed node and labeled training data, then you find the feature that best splits the data, next, each split partitions the node's input data. Repeat the process for each new node, each feature for once.\n",
    "\n",
    "You can set **tree pruning**. 1. Max depth is reached 2.min number of data points in a node has exceeded 3. min number of samples in a leaf 4. tree has reached max number of leaf nodes\n",
    "\n",
    "Best split uses two algo: Return highest information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c45de7-5722-4d1e-80e9-bbf24aa24f40",
   "metadata": {},
   "source": [
    "**Regression Tree** [Regression Tree](Regression-Trees-Taxi-Tip-v1.ipynb) target is continuous\n",
    "\n",
    "Objective: predict **continuous** target variable;  Using **Variance reduction (MSE) as splitting criterion**. LOWEST Weighted average of MSEs of each split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4472d07c-54c2-4dbc-b270-edfc2f2c0746",
   "metadata": {},
   "source": [
    "**Support Vector Machines** This is used for building classification and regression models. It classifies input by identifying hyperplane. Each input row is a coordinate in the hyperplane.\n",
    "\n",
    "Balance between maximizing margin controlled by parameter C. Smaller C allows more misclassifications (soft margin). Larger C is a harder margin. SVM is usually linear structure, but you can choose different kernels. **Linear  Polynomial  RBF  Sigmoid** \n",
    "\n",
    " [Support Vector Machine](decision-tree-svm-ccFraud-v1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f982c06-90d3-4eba-99c1-5da4afc4a196",
   "metadata": {},
   "source": [
    "**KNN**  [K Nearest Neighbor](KNN-lab-v1.ipynb) For each point, find the nearest K points and use the majority catagory as the prediction. KNN is a lazy learner, because it memorizes training data and makes predictions based on distance to training data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a24baa4-74fd-46ef-a034-51e3222626a0",
   "metadata": {},
   "source": [
    "**Bagging and Boosting** Bagging mitigates overfitting, using High variance low bias base learners, parallel training on bootstrapped data. Reduced overall variance.\n",
    "\n",
    "Boosting mitigates underfitting, using low variance high bias base learners, building on previous results, put more weight on wrong predictions. Gradually reducing bias.\n",
    "\n",
    "[Random Forests and XGBoost](Random-Forests-XGBoost-v1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f95646-8499-4719-8ff7-bd9f9534c01e",
   "metadata": {},
   "source": [
    "### Ch3 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d6498c-0cde-4f35-ba15-3c67e456b2a7",
   "metadata": {},
   "source": [
    "Classification is a supervised machine learning method used to predict labels on new data with applications in churn prediction, customer segmentation, loan default prediction, and multiclass drug prescriptions.\n",
    "\n",
    "Binary classifiers can be extended to multiclass classification using one-versus-all or one-versus-one strategies.\n",
    "\n",
    "A decision tree classifies data by testing features at each node, branching based on test results, and assigning classes at leaf nodes.\n",
    "\n",
    "Decision tree training involves selecting features that best split the data and pruning the tree to avoid overfitting.\n",
    "\n",
    "Information gain and Gini impurity are used to measure the quality of splits in decision trees.\n",
    "\n",
    "Regression trees are similar to decision trees but predict continuous values by recursively splitting data to maximize information gain.\n",
    "\n",
    "Mean Squared Error (MSE) is used to measure split quality in regression trees.\n",
    "\n",
    "K-Nearest Neighbors (k-NN) is a supervised algorithm used for classification and regression by assigning labels based on the closest labeled data points.\n",
    "\n",
    "To optimize k-NN, test various k values and measure accuracy, considering class distribution and feature relevance.\n",
    "\n",
    "Support Vector Machines (SVM) build classifiers by finding a hyperplane that maximizes the margin between two classes, effective in high-dimensional spaces but sensitive to noise and large datasets.\n",
    "\n",
    "The bias-variance tradeoff affects model accuracy, and methods such as bagging, boosting, and random forests help manage bias and variance to improve model performance.\n",
    "\n",
    "Random forests use bagging to train multiple decision trees on bootstrapped data, improving accuracy by reducing variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40768633-1882-434c-87d7-111b6a7ef93e",
   "metadata": {},
   "source": [
    "### CH4. Clustering and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505d49ba-46ba-4e6b-84b8-f52a8df4df68",
   "metadata": {},
   "source": [
    "**Partition-Based: Kmeans** k clusters with low variances\n",
    "**DBSCAN Density-Based** suitable for irregular clusters\n",
    "**Hierarchical clustering** Tree like structures, Divisive (Top down) and Agglomerative (Merge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955a2fc-c989-4076-aad6-9ee33532728c",
   "metadata": {},
   "source": [
    "**K means**: Initialization; Iteratively assgin points to clusters and update centroids; Repeat until centroids stop moving. But performs bad when noises exist. **Using Elbow method, DB index to determine K.**\n",
    "\n",
    "[K means](K-Means-Customer-Seg-v1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399bd2a1-fde0-421c-a1b4-8779ef0e358d",
   "metadata": {},
   "source": [
    "**DBSCAN (Density-Based Spatial Clustering of Applications with Noise):**\n",
    "\n",
    "It creates clusters based on a user-defined density value.\n",
    "It identifies core points (with enough neighbors), border points (near core points but not dense enough), and noise points (isolated).\n",
    "DBSCAN can discover clusters of various shapes and sizes and is effective in handling noise and outliers.\n",
    "\n",
    "Parameters: N (Minimum points required to form a dense region), epsilon (radius)\n",
    "\n",
    "How It Works:\n",
    "1. Finds core points with at least min_samples neighbors within eps.\n",
    "2. Expands clusters from core points, adding directly reachable points.\n",
    "3. Labels points as core, border, or noise.\n",
    "   \n",
    "**HDBSCAN** does not require any parameters to be set and uses cluster stability. Cluster stability is the persistence of a cluster over a range of distance thresholds. Automatically determines the optimal eps (no need to set it manually). Builds a hierarchical tree of clusters.\n",
    "More robust to parameter choices than DBSCAN.\n",
    "\n",
    "[Comparing-DBScan-HDBScan](Comparing-DBScan-HDBScan-v1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcad13d-a280-46a4-9308-f07834f7cbfb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Dimension Reduction Algorithm**\n",
    "\n",
    "UMAP (Uniform Manifold Approximation and Projection) and t-SNE (t-Distributed Stochastic Neighbor Embedding) are both non-linear dimensionality reduction techniques, primarily used for visualizing high-dimensional data (e.g., reducing from 100D to 2D or 3D). **UMAP is faster but math more complex**\n",
    "\n",
    "UMAP is designed to preserve local and global data structures, making it suitable for clustering applications.\n",
    "\n",
    "**PCA: Step-by-Step Linear Algebra Operations**\n",
    "\n",
    "**Step 1: Center Data**\n",
    "$$ X' = X - \\bar{X} $$\n",
    "\n",
    "**Step 2: Compute Covariance Matrix**\n",
    "$$ \\Sigma = \\frac{1}{m} X'^T X' $$\n",
    "\n",
    "**Step 3: Find Eigenvalues & Eigenvectors**\n",
    "$$ \\Sigma v = \\lambda v $$\n",
    "\n",
    "**Step 4: Select Top k Components**\n",
    "Sort by largest $$ \\lambda $$\n",
    "\n",
    "**Step 5: Project Data**\n",
    "Project the centered data onto the top \\( k \\) principal components:\n",
    "\n",
    "$$\n",
    "Z = X' W\n",
    "$$\n",
    "\n",
    "where:  \n",
    "\n",
    "- \\( X' \\) is the **mean-centered data matrix** of shape \\( m \\times n \\), where:\n",
    "  - \\( m \\) is the number of samples.\n",
    "  - \\( n \\) is the original feature dimension.\n",
    "- \\( W \\) is the **projection matrix**, consisting of the top \\( k \\) eigenvectors:\n",
    "  \n",
    "  $$\n",
    "  W = [v_1, v_2, ..., v_k], \\quad W \\in \\mathbb{R}^{n \\times k}\n",
    "  $$\n",
    "\n",
    "- \\( Z \\) is the **transformed dataset** in the reduced-dimensional space, with shape \\( m \\times k \\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a49ed4d-8d21-4705-a04e-832c8346015d",
   "metadata": {},
   "source": [
    "[PCA](PCA-v1.ipynb)\n",
    "\n",
    "[tSNE and UMAP](tSNE-UMAP-v1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d771e510-e0b8-4204-91e5-2793ea2e5d67",
   "metadata": {},
   "source": [
    "### CH4 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d434cef-1a87-43d1-bf5c-c04f720b5e1e",
   "metadata": {},
   "source": [
    "**Clustering** is a machine learning technique used to group data based on similarity, with applications in customer segmentation and anomaly detection.\n",
    "\n",
    "**K-means** clustering **partitions** data into clusters **based on the distance** between data points and centroids but struggles with imbalanced or non-convex clusters.\n",
    "\n",
    "Heuristic methods such as silhouette analysis, the elbow method, and the Davies-Bouldin Index help assess k-means performance.\n",
    "\n",
    "**DBSCAN is a density-based algorithm** that creates clusters based on density and works well with natural, irregular patterns.\n",
    "\n",
    "**HDBSCAN is a variant of DBSCAN** that does not require parameters and uses cluster stability to find clusters.\n",
    "\n",
    "Hierarchical clustering can be divisive (top-down) or agglomerative (bottom-up) and produces a dendrogram to visualize the cluster hierarchy.\n",
    "\n",
    "Dimension reduction simplifies data structure, improves clustering outcomes, and is useful in tasks such as face recognition (using eigenfaces).\n",
    "\n",
    "Clustering and dimension reduction work together to improve model performance by reducing noise and simplifying feature selection.\n",
    "\n",
    "PCA, a linear dimensionality reduction method, minimizes information loss while reducing dimensionality and noise in data.\n",
    "\n",
    "t-SNE and UMAP are other dimensionality reduction techniques that map high-dimensional data into lower-dimensional spaces for visualization and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b507e737-28f3-492b-93b7-b00b08b70d2d",
   "metadata": {},
   "source": [
    "### Ch5. Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411c39c5-5f4b-42d6-a623-efe9fb720f63",
   "metadata": {},
   "source": [
    "1. Accuracy: Ratio of correctly predicted instances to the total number of instances\n",
    "2. Confusion matrix: Breaks down ground truth instances of a class against the number of predicted class instances.\n",
    "3. Precision: Measures how many predicted positive instances are actually positive\n",
    "4. Recall: How many actual positive instances are correctly predicted\n",
    "5. F1 score: Harmonic Mean\n",
    "   \n",
    "[Metrics](EvaluatingClassificationModels-v1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca05f7-557b-45fd-a975-ed4bae953d78",
   "metadata": {},
   "source": [
    "Key Metrics:\n",
    "\n",
    "1. MAE (Mean Absolute Error): Average absolute difference between predicted and observed values.\n",
    "\n",
    "2. MSE (Mean Squared Error): Sum of squared differences between predicted and observed values, divided by the number of observations minus parameters.\n",
    "\n",
    "3. RMSE (Root Mean Squared Error): Square root of MSE, easier to interpret as it shares the same units as the target variable.\n",
    "\n",
    "4. R-squared: Measures the proportion of variance in the dependent variable explained by the independent variable, ranging from 0 (poor fit) to 1 (perfect fit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82515a9-6aca-4390-b37f-3f5d5990792e",
   "metadata": {},
   "source": [
    "[Evaluating-random-forest](Evaluating-random-forest-v1.ipynb)\n",
    "\n",
    "[Evaluating-Clustering](Evaluating-k-means-clustering-v1.ipynb)\n",
    "\n",
    "The Silhouette score measures how similar each point is to its own cluster compared to other clusters, providing an evaluation of the quality and separation of clusters. This makes it most suitable for assessing how well the clusters are formed in terms of separation and cohesion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8459e34-e463-48c7-821c-e8a090f041ff",
   "metadata": {},
   "source": [
    "**K-fold cross-validation**: for each fold, train on remaining k-1 folds, test on fold and store model score. Compute the aggregated score. Select the best model.\n",
    "\n",
    "For imbalanced dataset, using **stratified cross-validation**.\n",
    "\n",
    "**Regularization** regularized cost function = MSE + lambda * penalty  \n",
    "1. **Ridge L2, lambda*sum(parameters^2)**\n",
    "2. **LASSO L1, lambda*sum(abs(parameter))** sparse coefficients; best for low SNR conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708cc9de-ee01-44a7-90b9-51b0905c5a90",
   "metadata": {},
   "source": [
    "### CH5 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49893272-3118-4237-90a3-3be7c39e121e",
   "metadata": {},
   "source": [
    "Supervised learning evaluation assesses a model's ability to predict outcomes for unseen data, often using a train/test split to estimate performance.\n",
    "\n",
    "Key metrics for classification evaluation include accuracy, confusion matrix, precision, recall, and the F1 score, which balances precision and recall.\n",
    "\n",
    "Regression model evaluation metrics include MAE, MSE, RMSE, R-squared, and explained variance to measure prediction accuracy.\n",
    "\n",
    "Unsupervised learning models are evaluated for pattern quality and consistency using metrics like Silhouette Score, Davies-Bouldin Index, and Adjusted Rand Index.\n",
    "\n",
    "Dimensionality reduction evaluation involves Explained Variance Ratio, Reconstruction Error, and Neighborhood Preservation to assess data structure retention.\n",
    "\n",
    "Model validation, including dividing data into training, validation, and test sets, helps prevent overfitting by tuning hyperparameters carefully.\n",
    "\n",
    "Cross-validation methods, especially K-fold and stratified cross-validation, support robust model validation without overfitting to test data.\n",
    "\n",
    "Regularization techniques, such as ridge (L2) and lasso (L1) regression, help prevent overfitting by adding penalty terms to linear regression models.\n",
    "\n",
    "Data leakage occurs when training data includes information unavailable in real-world data, which is preventable by separating data properly and mindful feature selection.\n",
    "\n",
    "Common modeling pitfalls include misinterpreting feature importance, ignoring class imbalance, and relying excessively on automated processes without causal analysis.\n",
    "\n",
    "Feature importance assessments should consider redundancy, scale sensitivity, and avoid misinterpretation, as well as inappropriate assumptions about causation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099442ec-785e-4b3b-85d5-1e405a7fbd91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
